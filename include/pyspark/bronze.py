import great_expectations_common as gec
import os
from pathlib import Path
from pyspark.sql import functions as F

class Bronze():
    def __init__(self, spark_session, db_setup_manager):
        self.spark = spark_session
        self.manager = db_setup_manager
        
        # 1. è·¯å¾„è‡ªé€‚åº”
        if os.environ.get('AIRFLOW_HOME'):
            # Airflow å®¹å™¨ç¯å¢ƒ
            root = Path("/usr/local/airflow/include")
        else:
            # æœ¬åœ°ç¯å¢ƒ
            root = Path(__file__).resolve().parent.parent

        # æ•°æ®æºè·¯å¾„ï¼šæ˜ç¡®åˆ°å…·ä½“çš„ä¸»é¢˜æ–‡ä»¶å¤¹
        self.raw_data_base = root / "data"
        
        # Checkpoint è·¯å¾„å»ºè®®æ”¹ä¸º MinIOï¼Œç¡®ä¿æŒä¹…åŒ–
        # ä¹Ÿå¯ä»¥ä¿ç•™æœ¬åœ°ï¼Œä½†è¦ç¡®ä¿å®¿ä¸»æœºæŒ‚è½½äº†è¯¥å·
        self.checkpoint_base = f"s3a://{self.manager.bucket}/_checkpoints"

        print(f"ğŸ“‚ Bronze æ•°æ®åŸºå‡†è·¯å¾„: {self.raw_data_base}")
        print(f"ğŸ’¾ Checkpoint æ ¹è·¯å¾„: {self.checkpoint_base}")

    def consume_reddit_posts_bz(self, once=True, processing_time="5 seconds"):
        # æ˜ç¡®å­è·¯å¾„
        data_path = str(self.raw_data_base)
        
        schema = '''
                post_id STRING, 
                title STRING, 
                author STRING, 
                score INT,
                upvote_ratio DOUBLE, 
                comments INT, 
                flair STRING, 
                is_video STRING, 
                is_self STRING, 
                domain STRING, 
                url STRING,
                created_utc STRING, 
                selftext STRING,
                extracted_time TIMESTAMP
        '''
        
        # æ£€æŸ¥æœ¬åœ°è·¯å¾„æ˜¯å¦å­˜åœ¨ï¼Œé˜²æ­¢ readStream ç«‹å³æŠ¥é”™
        if not os.path.exists(data_path):
            os.makedirs(data_path, exist_ok=True)
            print(f"âš ï¸ Warning: è·¯å¾„ {data_path} ä¸ºç©ºï¼Œå·²è‡ªåŠ¨åˆ›å»ºã€‚")

        df_stream = (self.spark.readStream
                        .format("csv")
                        .schema(schema)
                        .option("header", "true")
                        .option("recursiveFileLookup", "true") 
                        .option("pathGlobFilter", "*.csv")
                        .option("maxFilesPerTrigger", 10) 
                        .load(data_path)
                        .withColumn("load_time", F.current_timestamp())
                    )
        
        # ä¼ å…¥ table_name = "reddit_posts_bz"
        return self._write_stream_append(
            df_stream, 
            "reddit_posts_bz", 
            "reddit_posts_bz_ingestion_stream", 
            "bronze_p1", 
            once, 
            processing_time
        )

    def _write_stream_append(self, df, table_name, query_name, pool, once, processing_time):
        # æ„é€  Checkpoint è·¯å¾„
        checkpoint_path = f"{self.checkpoint_base}/{table_name}"
        
        # å†…éƒ¨å®šä¹‰ Batch å¤„ç†é€»è¾‘ï¼Œç¡®ä¿åºåˆ—åŒ–å®‰å…¨
        manager_instance = self.manager # å±€éƒ¨å¼•ç”¨
        def batch_processor(micro_df, batch_id):
            gec.validate_and_insert_process_batch(
                micro_df, 
                batch_id, 
                table_name, 
                manager_instance
            )

        stream_writer = (df.writeStream
            .foreachBatch(batch_processor)
            .option("checkpointLocation", checkpoint_path)
            .queryName(query_name)
        ) 

        self.spark.sparkContext.setLocalProperty("spark.scheduler.pool", pool)
        
        if once:
            return stream_writer.trigger(availableNow=True).start()
        else:
            return stream_writer.trigger(processingTime=processing_time).start()

    def consume(self, once=True, processing_time="5 seconds"):
        import time
        start = time.time()
        print(f"\nğŸš€ Starting bronze layer consumption ...")
        
        # è·å– Active Stream
        stream = self.consume_reddit_posts_bz(once, processing_time)
        
        if once:
            # è¿™ç§æ–¹å¼æ¯”éå†æ‰€æœ‰ active streams æ›´ç²¾å‡†
            stream.awaitTermination()
                
        print(f"âœ… Completed bronze layer consumption in {int(time.time() - start)} seconds")