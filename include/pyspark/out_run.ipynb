{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b006305",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T00:00:05.461136Z",
     "iopub.status.busy": "2026-01-15T00:00:05.460915Z",
     "iopub.status.idle": "2026-01-15T00:00:05.470970Z",
     "shell.execute_reply": "2026-01-15T00:00:05.470374Z"
    },
    "papermill": {
     "duration": 0.014785,
     "end_time": "2026-01-15T00:00:05.471754",
     "exception": false,
     "start_time": "2026-01-15T00:00:05.456969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 1. è‡ªåŠ¨å¤„ç†è·¯å¾„ä¸ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "current_path = os.getcwd()\n",
    "if current_path not in sys.path:\n",
    "    sys.path.append(current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23debc89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T00:00:05.474829Z",
     "iopub.status.busy": "2026-01-15T00:00:05.474729Z",
     "iopub.status.idle": "2026-01-15T00:00:07.204717Z",
     "shell.execute_reply": "2026-01-15T00:00:07.204326Z"
    },
    "papermill": {
     "duration": 1.731654,
     "end_time": "2026-01-15T00:00:07.205025",
     "exception": false,
     "start_time": "2026-01-15T00:00:05.473371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Preloaded GX Suite: reddit_posts_bz_suite\n"
     ]
    }
   ],
   "source": [
    "from spark_common import MinIOSparkManager\n",
    "from setup import LakehouseSetupManager\n",
    "from bronze import Bronze\n",
    "from silver import Silver\n",
    "from gold import Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "962c89a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T00:00:07.207083Z",
     "iopub.status.busy": "2026-01-15T00:00:07.206910Z",
     "iopub.status.idle": "2026-01-15T00:00:07.209907Z",
     "shell.execute_reply": "2026-01-15T00:00:07.209558Z"
    },
    "papermill": {
     "duration": 0.004202,
     "end_time": "2026-01-15T00:00:07.210155",
     "exception": false,
     "start_time": "2026-01-15T00:00:07.205953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "once = True \n",
    "processing_time = \"5 seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96873f44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T00:00:07.211672Z",
     "iopub.status.busy": "2026-01-15T00:00:07.211615Z",
     "iopub.status.idle": "2026-01-15T00:00:07.213831Z",
     "shell.execute_reply": "2026-01-15T00:00:07.213565Z"
    },
    "papermill": {
     "duration": 0.003301,
     "end_time": "2026-01-15T00:00:07.214133",
     "exception": false,
     "start_time": "2026-01-15T00:00:07.210832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. åˆå§‹åŒ– Manager\n",
    "manager = MinIOSparkManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ea46dec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T00:00:07.215631Z",
     "iopub.status.busy": "2026-01-15T00:00:07.215576Z",
     "iopub.status.idle": "2026-01-15T00:00:10.015281Z",
     "shell.execute_reply": "2026-01-15T00:00:10.014553Z"
    },
    "papermill": {
     "duration": 2.802027,
     "end_time": "2026-01-15T00:00:10.016822",
     "exception": false,
     "start_time": "2026-01-15T00:00:07.214795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/astro/.ivy2/cache\n",
      "The jars for the packages stored in: /home/astro/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a7179f1a-2aec-4ed5-b300-44c4e1efe4b8;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.2.1 in central\n",
      "\tfound io.delta#delta-storage;3.2.1 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 101ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.2.1 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.1 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   0   |   0   |   0   ||   6   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a7179f1a-2aec-4ed5-b300-44c4e1efe4b8\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 6 already retrieved (0kB/2ms)\n",
      "26/01/15 00:00:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# 2. å¯åŠ¨ Spark\n",
    "spark = manager.create_session(\"Reddit_ETL_Pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc86bf3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T00:00:10.020894Z",
     "iopub.status.busy": "2026-01-15T00:00:10.020774Z",
     "iopub.status.idle": "2026-01-15T00:00:38.971730Z",
     "shell.execute_reply": "2026-01-15T00:00:38.969997Z"
    },
    "papermill": {
     "duration": 28.953948,
     "end_time": "2026-01-15T00:00:38.973250",
     "exception": false,
     "start_time": "2026-01-15T00:00:10.019302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹å…¨é“¾è·¯ ETL æµç¨‹ (Mode: Batch)\n",
      "ğŸ“ ç›®æ ‡å­˜å‚¨: http://minio:9000 / reddit-warehouse\n",
      "\n",
      "ğŸ“¦ [STAGE: BRONZE] æ­£åœ¨åŒæ­¥æ•°æ®è‡³åŸå§‹å±‚...\n",
      "ğŸ“‚ Bronze æ•°æ®åŸºå‡†è·¯å¾„: /usr/local/airflow/include/data\n",
      "ğŸ’¾ Checkpoint æ ¹è·¯å¾„: s3a://reddit-warehouse/_checkpoints\n",
      "\n",
      "ğŸš€ Starting bronze layer consumption ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/15 00:00:10 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/15 00:00:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/15 00:00:12 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Batch 2: Running GX for reddit_posts_bz...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating Metrics:   0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating Metrics:   0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating Metrics:   7%|â–‹         | 4/56 [00:00<00:00, 853.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating Metrics:   7%|â–‹         | 4/56 [00:00<00:00, 805.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating Metrics:  14%|â–ˆâ–        | 8/56 [00:00<00:01, 38.62it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating Metrics:  14%|â–ˆâ–        | 8/56 [00:00<00:01, 38.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating Metrics:  14%|â–ˆâ–        | 8/56 [00:00<00:01, 38.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating Metrics:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/56 [00:00<00:00, 38.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating Metrics:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/56 [00:00<00:00, 38.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating Metrics:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 46/56 [00:01<00:00, 34.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating Metrics:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 46/56 [00:01<00:00, 34.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating Metrics:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 46/56 [00:01<00:00, 34.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating Metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56/56 [00:01<00:00, 35.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating Metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56/56 [00:01<00:00, 35.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating Metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56/56 [00:01<00:00, 35.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating Metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56/56 [00:01<00:00, 35.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating Metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56/56 [00:01<00:00, 35.75it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Batch 2: Passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed bronze layer consumption in 6 seconds\n",
      "\n",
      "ğŸ¥ˆ [STAGE: SILVER] æ­£åœ¨æ¸…æ´—å¹¶è½¬æ¢è‡³ç™½é“¶å±‚...\n",
      "ğŸ’¾ Silver å±‚ Checkpoint æ ¹è·¯å¾„: s3a://reddit-warehouse/_checkpoints/silver\n",
      "\n",
      "ğŸš€ å¯åŠ¨ Silver å±‚ CDC Upsert...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/15 00:00:16 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 29:================================================>     (181 + 5) / 200]\r",
      "\r",
      "                                                                                \r",
      "26/01/15 00:00:18 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 31:========================================>               (36 + 5) / 50]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 36:==================================>                       (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 40:=======>                                               (28 + 5) / 200]\r",
      "\r",
      "[Stage 40:============>                                          (44 + 5) / 200]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 40:================>                                      (60 + 5) / 200]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 40:====================>                                  (75 + 5) / 200]\r",
      "\r",
      "[Stage 40:========================>                              (90 + 5) / 200]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 40:===========================>                          (103 + 5) / 200]\r",
      "\r",
      "[Stage 40:================================>                     (121 + 5) / 200]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 40:=====================================>                (138 + 5) / 200]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 40:==========================================>           (158 + 5) / 200]\r",
      "\r",
      "[Stage 40:===============================================>      (177 + 5) / 200]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 40:=====================================================>(199 + 1) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Silver å±‚æ›´æ–°å®Œæˆï¼Œè€—æ—¶: 8 ç§’\n",
      "\n",
      "ğŸ¥‡ [STAGE: GOLD] æ­£åœ¨èšåˆå¹¶å»é‡è‡³é»„é‡‘å±‚...\n",
      "ğŸ’¾ Gold Checkpoint æ ¹è·¯å¾„: s3a://reddit-warehouse/_checkpoints\n",
      "\n",
      "ğŸš€ æ‰§è¡Œ Gold å±‚ Upsert (è‡ªåŒ…å«æ¨¡å¼) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/15 00:00:24 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 60:==================>                                    (69 + 6) / 200]\r",
      "\r",
      "[Stage 60:==========================>                            (95 + 6) / 200]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 60:=================================>                    (123 + 6) / 200]\r",
      "\r",
      "[Stage 60:==========================================>           (157 + 5) / 200]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 60:================================================>     (180 + 5) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 75:==============================>                       (112 + 5) / 200]\r",
      "\r",
      "[Stage 75:==========================================>           (156 + 5) / 200]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 75:====================================================> (193 + 6) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 90:============================================>         (164 + 6) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 105:=============================================>       (171 + 5) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å®Œæˆ Gold å±‚åŠ å·¥ï¼Œè€—æ—¶ 13 ç§’\n",
      "\n",
      "âœ¨ æ‰€æœ‰å±‚çº§å¤„ç†å®Œæˆï¼\n",
      "ğŸ›‘ æ­£åœ¨å…³é—­ Spark Session...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # 4. åˆå§‹åŒ– Setup Manager (ç¯å¢ƒæ³¨å…¥)\n",
    "    # ç»Ÿä¸€ä½¿ç”¨ manager å·²ç»ä» .env ä¸­è§£æå¥½çš„é…ç½®ï¼Œæ¶ˆé™¤ localhost/minio çš„å†²çª\n",
    "    DBsetup = LakehouseSetupManager(\n",
    "        spark_session=spark,\n",
    "        endpoint=manager.endpoint,\n",
    "        access_key=manager.access_key,\n",
    "        secret_key=manager.secret_key,\n",
    "        bucket=manager.bucket\n",
    "    )\n",
    "\n",
    "    print(f\"ğŸš€ å¼€å§‹å…¨é“¾è·¯ ETL æµç¨‹ (Mode: {'Batch' if once else 'Streaming'})\")\n",
    "    print(f\"ğŸ“ ç›®æ ‡å­˜å‚¨: {manager.endpoint} / {manager.bucket}\")\n",
    "\n",
    "    # --- é˜¶æ®µ 1: Bronze (Raw -> Delta) ---\n",
    "    print(\"\\nğŸ“¦ [STAGE: BRONZE] æ­£åœ¨åŒæ­¥æ•°æ®è‡³åŸå§‹å±‚...\")\n",
    "    bronze = Bronze(spark, DBsetup)\n",
    "    bronze.consume(once, processing_time)\n",
    "\n",
    "    # --- é˜¶æ®µ 2: Silver (Filter & Cast -> Delta) ---\n",
    "    print(\"\\nğŸ¥ˆ [STAGE: SILVER] æ­£åœ¨æ¸…æ´—å¹¶è½¬æ¢è‡³ç™½é“¶å±‚...\")\n",
    "    silver = Silver(spark, DBsetup)\n",
    "    silver.upsert(once, processing_time)\n",
    "\n",
    "    # --- é˜¶æ®µ 3: Gold (CDC & Deduplication -> Delta) ---\n",
    "    print(\"\\nğŸ¥‡ [STAGE: GOLD] æ­£åœ¨èšåˆå¹¶å»é‡è‡³é»„é‡‘å±‚...\")\n",
    "    gold = Gold(spark, DBsetup)\n",
    "    gold.upsert(once, processing_time)\n",
    "\n",
    "    print(\"\\nâœ¨ æ‰€æœ‰å±‚çº§å¤„ç†å®Œæˆï¼\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ æµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    # 5. ç¡®ä¿ Spark èµ„æºè¢«é‡Šæ”¾\n",
    "    print(\"ğŸ›‘ æ­£åœ¨å…³é—­ Spark Session...\")\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 36.827102,
   "end_time": "2026-01-15T00:00:41.601773",
   "environment_variables": {},
   "exception": null,
   "input_path": "run.ipynb",
   "output_path": "out_run.ipynb",
   "parameters": {},
   "start_time": "2026-01-15T00:00:04.774671",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}