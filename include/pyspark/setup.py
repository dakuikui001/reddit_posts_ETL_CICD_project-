import os
import time
from pathlib import Path
from dotenv import load_dotenv
from trino.dbapi import connect
from spark_common import MinIOSparkManager 

class LakehouseSetupManager():
    def __init__(self, spark_session, endpoint=None, access_key=None, secret_key=None, 
                 bucket=None, trino_host=None, trino_port=None):
        
        # 1. è‡ªåŠ¨åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv()
        
        # 2. åŸºç¡€é…ç½®ä¼˜å…ˆçº§ï¼šæ˜¾å¼ä¼ å…¥ > ç¯å¢ƒå˜é‡ > é»˜è®¤å€¼
        self.spark = spark_session
        self.endpoint = endpoint or os.getenv("AWS_ENDPOINT_URL")
        self.access_key = access_key or os.getenv("AWS_ACCESS_KEY_ID")
        self.secret_key = secret_key or os.getenv("AWS_SECRET_ACCESS_KEY")
        self.bucket = bucket or os.getenv("MINIO_BUCKET")
        
        # Trino é…ç½® (Astro ç¯å¢ƒä¸‹ host é€šå¸¸ä¸º 'trino')
        self.trino_host = trino_host or os.getenv("TRINO_HOST") # Astro ç¯å¢ƒé»˜è®¤ä¸º trino
        self.trino_port = int(trino_port or os.getenv("TRINO_PORT")) # å®¹å™¨å†…éƒ¨é€šå¸¸æ˜¯ 8080
        
        self.db_name = "reddit_db"
        
        # åˆå§‹åŒ– MinIO Manager (å¤ç”¨å‚æ•°)
        self.manager = MinIOSparkManager(
            endpoint=self.endpoint, 
            access_key=self.access_key, 
            secret_key=self.secret_key, 
            bucket=self.bucket
        )
        
        self.trino_config = {
            "host": self.trino_host,
            "port": self.trino_port,
            "user": "admin",
            "catalog": "delta"
        }
        
        # Checkpoint è·¯å¾„å»ºè®®ä¹Ÿæ”¾å…¥ S3 ç»Ÿä¸€ç®¡ç†ï¼Œé¿å…æœ¬åœ°è·¯å¾„æƒé™é—®é¢˜
        self.checkpoint_base = f"s3a://{self.bucket}/_checkpoints"

    # --- è¾…åŠ©æ–¹æ³• ---
    def _get_table_location(self, table_name, protocol="s3a"):
        """ç”Ÿæˆç‰©ç†è·¯å¾„ã€‚Spark ç”¨ s3a://, Trino ç”¨ s3://"""
        return f"{protocol}://{self.bucket}/{self.db_name}/{table_name}"

    def execute_trino_raw_sql(self, sql):
        """æ‰§è¡Œ Trino DDLï¼Œå¢åŠ è¿æ¥è¶…æ—¶å¤„ç†"""
        try:
            with connect(**self.trino_config) as conn:
                cur = conn.cursor()
                cur.execute(sql)
                # fetchone é˜²æ­¢æŸäº›é©±åŠ¨éœ€è¦æ˜¾å¼æ¶ˆè€—ç»“æœé›†
                return True
        except Exception as e:
            print(f"âš ï¸ Trino SQL æç¤º (éè‡´å‘½): {e}")
            return False

    # --- ä¸»æµç¨‹ ---
    def setup(self):
        print(f"ğŸ—ï¸ æ­£åœ¨ä»é›¶å¼€å§‹åˆå§‹åŒ– Lakehouse...")
        # 1. ç¡®ä¿æ•°æ®åº“å­˜åœ¨
        self.spark.sql(f"CREATE DATABASE IF NOT EXISTS {self.db_name}")

        table_definitions = {
            "reddit_posts_bz": "post_id STRING, title STRING, author STRING, score INT, upvote_ratio DOUBLE, comments INT, flair STRING, is_video STRING, is_self STRING, domain STRING, url STRING, created_utc STRING, selftext STRING, extracted_time TIMESTAMP, load_time TIMESTAMP",
            "reddit_posts_sl": "post_id STRING, title STRING, author STRING, score INT, upvote_ratio DOUBLE, comments INT, flair STRING, is_video BOOLEAN, is_self BOOLEAN, domain STRING, url STRING, created_utc TIMESTAMP, selftext STRING, extracted_time TIMESTAMP, load_time TIMESTAMP, update_time TIMESTAMP",
            "fact_posts_gl": "post_id STRING, title STRING, author STRING, score INT, upvote_ratio DOUBLE, comments INT, flair STRING, domain STRING, format STRING, url STRING, created_utc TIMESTAMP, selftext STRING, extracted_time TIMESTAMP, update_time TIMESTAMP",
            "dim_authors_gl": "author STRING, update_time TIMESTAMP",
            "dim_flairs_gl": "flair STRING, update_time TIMESTAMP",
            "dim_domains_gl": "domain STRING, update_time TIMESTAMP",
            "data_quality_quarantine": "table_name STRING, gx_batch_id STRING, violated_rules STRING, raw_data STRING, ingestion_time TIMESTAMP"
        }

        # 3. ç‰©ç†å»ºè¡¨å¾ªç¯
        for t, schema_sql in table_definitions.items():
            location = self._get_table_location(t, "s3a")
            
            # å…ˆåˆ æ‰æ—§çš„å…ƒæ•°æ®
            self.spark.sql(f"DROP TABLE IF EXISTS {self.db_name}.{t}")

            # å¦‚æœæ˜¯ Silver è¡¨ï¼Œç›´æ¥åœ¨åˆ›å»ºæ—¶å¼€å¯ CDC
            tbl_props = ""
            if t == "reddit_posts_sl":
                tbl_props = "TBLPROPERTIES (delta.enableChangeDataFeed = true)"

            # ä½¿ç”¨ æ­£ç¡®çš„æ•°æ®åº“å.è¡¨å
            create_sql = f"""
                CREATE TABLE {self.db_name}.{t} ({schema_sql}) 
                USING DELTA 
                LOCATION '{location}'
                {tbl_props}
            """
            self.spark.sql(create_sql)
            print(f"âœ… Spark ç‰©ç†è¡¨å·²å°±ç»ª: {self.db_name}.{t} {'(CDC å·²å¼€å¯)' if tbl_props else ''}")

        # 4. Trino æ³¨å†Œ (æ³¨æ„ï¼šTrino é‡Œæˆ‘ä»¬æ‰‹åŠ¨åŠ ä¸Š delta å‰ç¼€)
        from trino.dbapi import connect
        with connect(**self.trino_config) as conn:
            cur = conn.cursor()
            cur.execute(f"CREATE SCHEMA IF NOT EXISTS delta.{self.db_name}")
            for t in table_definitions.keys():
                cur.execute(f"DROP TABLE IF EXISTS delta.{self.db_name}.{t}")
                location_trino = self._get_table_location(t, "s3")
                cur.execute(f"CALL delta.system.register_table('{self.db_name}', '{t}', '{location_trino}')")
                print(f"âœ… Trino æ³¨å†ŒæˆåŠŸ: delta.{self.db_name}.{t}")

        print("ğŸš€ æ‰€æœ‰è¡¨åˆå§‹åŒ–å®Œæˆï¼")

    def cleanup(self):
        print(f"ğŸ§¹ æ‰§è¡Œå…¨é“¾è·¯æ¸…ç†...")
        tables = ["reddit_posts_bz", "reddit_posts_sl", "fact_posts_gl", 
                  "dim_authors_gl", "dim_flairs_gl", "dim_domains_gl", "data_quality_quarantine"]
        
        # 1. æ¸…ç† Trino
        for t in tables:
            self.execute_trino_raw_sql(f"DROP TABLE IF EXISTS delta.{self.db_name}.{t}")
        self.execute_trino_raw_sql(f"DROP SCHEMA IF EXISTS delta.{self.db_name}")

        # 2. æ¸…ç† Spark
        for t in tables:
            self.spark.sql(f"DROP TABLE IF EXISTS {self.db_name}.{t}")
        self.spark.sql(f"DROP DATABASE IF EXISTS {self.db_name} CASCADE")

        # 3. ç‰©ç†åˆ é™¤ (MinIO ä¸Šçš„æ•°æ® + Checkpoints)
        db_path = f"s3a://{self.bucket}/{self.db_name}"
        self._delete_s3_path(db_path)
        self._delete_s3_path(self.checkpoint_base)
        
        print("âœ¨ ç¯å¢ƒæ¸…ç†å®Œæˆã€‚")

    def _delete_s3_path(self, s3_path):
        """åº•å±‚ Hadoop API åˆ é™¤ S3 è·¯å¾„"""
        try:
            sc = self.spark.sparkContext
            Path_class = sc._gateway.jvm.org.apache.hadoop.fs.Path
            FileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem
            conf = sc._jsc.hadoopConfiguration()
            
            uri = sc._gateway.jvm.java.net.URI(s3_path)
            fs = FileSystem.get(uri, conf)
            fs.delete(Path_class(s3_path), True)
            print(f"âœ… ç‰©ç†è·¯å¾„å·²åˆ é™¤: {s3_path}")
        except Exception as e:
            print(f"â„¹ï¸ ç‰©ç†è·¯å¾„è·³è¿‡ (å¯èƒ½å·²ç©º): {s3_path}")

    def validate(self):
        print("\nğŸ” æ­£åœ¨éªŒè¯ Lakehouse çŠ¶æ€:")
        for table in ["reddit_posts_bz", "reddit_posts_sl", "fact_posts_gl"]:
            exists = self.spark.catalog.tableExists(f"{self.db_name}.{table}")
            print(f"{'âœ…' if exists else 'âŒ'} {table}")